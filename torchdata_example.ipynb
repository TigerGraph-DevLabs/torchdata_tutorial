{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d906cb6-883e-4007-84db-c0b8cdc575d4",
   "metadata": {},
   "source": [
    "# Exploring how the TorchData API works with TigerGraph data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba21f58-89a4-46d0-98d5-eec4247b12a8",
   "metadata": {},
   "source": [
    "### Doris Voina (dorisvoina@gmail.com), Feng Shi (bill.shi@tigergraph.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d342d6-5d93-4c5a-983f-bff2ae0fffd6",
   "metadata": {},
   "source": [
    "## What is TorchData?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b300d0b",
   "metadata": {},
   "source": [
    "TorchData (or \"torchdata\") is part of PyTorch project and provides better data loading functions for pytorch. According to the [documentation](https://pytorch.org/data/main/), \n",
    ">TorchData is a library of common modular data loading primitives for easily constructing flexible and performant data pipelines.\n",
    "\n",
    "The modular data loading primitives of torchdata are capable of accomplishing a variety of functions:\n",
    "- FileLister: lists out files in a directory\n",
    "- Filter: filters the elements in DataPipe based on a given function\n",
    "- FileOpener: consumes file paths and returns opened file streams\n",
    "- Mapper: Applies a function over each item from the source DataPipe \n",
    "\n",
    "This tutorial showcases how to use TorchData to load data from a TigerGraph database for graph machine learning tasks. The Cora dataset will used here, which is a well-known graph dataset of papers and their citations.\n",
    "\n",
    "Let's use torchdata by looking at a particular problem: \n",
    "starting with a graph, a common problem is classifying nodes of the graph. A common approach is to consider node features and then classify nodes according to these features, using say a neural network. While we can use the features provided in the dataset, we can further enrich these features by adding node properties in the graph, such as pagerank, a property whereby the number and quality of conenctions to a node are counted in order to determine a rough estimate of how important the node is. The underlying assumption of pagerank is that more important nodes are likely to receive more connections from other nodes.  \n",
    "\n",
    "**Outline**\n",
    "- Data Ingestion: Ingest the Cora dataset into a TigerGraph database on cloud. \n",
    "- Graph Features: Compute PageRank scores for each node in the database.\n",
    "- Data Loader: Create a data loader that preprocesses the data for ML.\n",
    "- Model Training: Train a simple feedforward neural network on the data for node classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5076294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdata in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (0.4.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from torchdata) (2.5.1)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from torchdata) (1.12.1)\n",
      "Requirement already satisfied: requests in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from torchdata) (2.28.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from torchdata) (1.26.11)\n",
      "Requirement already satisfied: typing-extensions in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from torch==1.12.1->torchdata) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from requests->torchdata) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from requests->torchdata) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/feng.shi/mlworkbench/lib/python3.9/site-packages (from requests->torchdata) (2022.6.15.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# A few python packages are requred for running the code below\n",
    "%pip install torchdata\n",
    "%pip install pyTigerGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e39f5-debb-4dd8-8cca-d321b96e7e3f",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae6fba-2fc5-42b8-b010-67041f53c5fb",
   "metadata": {},
   "source": [
    "We will use a free TigerGraph database on tgcloud to host the data. If you don't have a tgcloud account, simply go to www.tgcloud.io and get one. \n",
    "\n",
    "After logged in, click `Create Solution` and follow the instructions to create a database. (The free tier is enough for this demo although larger instances will bring better performance.) A solution called `torchdata-demo` at `torchdata-demo.i.tgcloud.io` is used in this tutorial. \n",
    "\n",
    "After the solution is created, open GraphStudio from `Applications`, and click `Global View`->`Create a graph` to create an empty graph called `Cora`.   \n",
    "\n",
    "Finally, click `Admin` on the upper right corner to switch to the admin portal and go to `Management/Users` to create a secret for the graph. Note: you will not be able to see the full secret once you leave the page; make sure to copy it to a safe place for future uses.\n",
    "\n",
    "Run the code below to ingest the Cora data into the database. **Replace the secret variable below with the secret generated just now**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e54f44-6097-47a2-988c-af8fef08b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a connection\n",
    "\n",
    "from pyTigerGraph import TigerGraphConnection\n",
    "\n",
    "secret = \"udnte2efngcbsdvjalkp3r90lqer0n9h\"\n",
    "\n",
    "conn = TigerGraphConnection(\n",
    "    host=\"https://torchdata-demo.i.tgcloud.io\",\n",
    "    graphname=\"Cora\",\n",
    "    gsqlSecret=secret,\n",
    ")\n",
    "\n",
    "apiToken = conn.getToken(secret)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7aa47e-d4a4-4e9b-87c3-01cf3cbdefbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using graph 'Cora'\n",
      "Successfully created schema change jobs: [Cora_job].\n",
      "Kick off schema change job Cora_job\n",
      "Doing schema change on graph 'Cora' (current version: 0)\n",
      "Trying to add local vertex 'Paper' to the graph 'Cora'.\n",
      "Trying to add local edge 'Cite' to the graph 'Cora'.\n",
      "\n",
      "Graph Cora updated to new version 1\n",
      "The job Cora_job completes in 2.358 seconds!\n"
     ]
    }
   ],
   "source": [
    "# Create and run schema change job\n",
    "query = \"\"\"\n",
    "USE GRAPH Cora\n",
    "\n",
    "CREATE SCHEMA_CHANGE JOB Cora_job FOR GRAPH Cora {\n",
    "    ADD VERTEX Paper (PRIMARY_ID id Int, x List<Int>, y Int, train_mask Bool, val_mask Bool, test_mask Bool) WITH primary_id_as_attribute=\"true\";\n",
    "    ADD DIRECTED EDGE Cite (from Paper, to Paper, time Int, is_train Bool, is_val Bool);\n",
    "}\n",
    "\n",
    "RUN SCHEMA_CHANGE JOB Cora_job\n",
    "\"\"\"\n",
    "print(conn.gsql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "797eebb4-c555-4f4a-b282-7163412d0fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using graph 'Cora'\n",
      "Successfully created loading jobs: [load_cora_data].\n"
     ]
    }
   ],
   "source": [
    "# Create loading job\n",
    "query = \"\"\"\n",
    "USE GRAPH Cora\n",
    "\n",
    "CREATE LOADING JOB load_cora_data FOR GRAPH Cora {\n",
    "    DEFINE FILENAME node_csv;\n",
    "    DEFINE FILENAME edge_csv;\n",
    "\n",
    "    LOAD node_csv TO VERTEX Paper VALUES ($0, SPLIT($1,\" \"), $2, $3, $4, $5) USING header=\"false\", separator=\",\";\n",
    "    LOAD edge_csv TO EDGE Cite VALUES ($0, $1, _, _, _) USING header=\"false\", separator=\",\";\n",
    "}\n",
    "\"\"\"\n",
    "print(conn.gsql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861706bb-2490-45c6-a917-c5252030df81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sourceFileName': 'Online_POST',\n",
       "  'statistics': {'validLine': 10556,\n",
       "   'rejectLine': 0,\n",
       "   'failedConditionLine': 0,\n",
       "   'notEnoughToken': 0,\n",
       "   'invalidJson': 0,\n",
       "   'oversizeToken': 0,\n",
       "   'vertex': [],\n",
       "   'edge': [{'typeName': 'Cite',\n",
       "     'validObject': 10556,\n",
       "     'noIdFound': 0,\n",
       "     'invalidAttribute': 0,\n",
       "     'invalidVertexType': 0,\n",
       "     'invalidPrimaryId': 0,\n",
       "     'invalidSecondaryId': 0,\n",
       "     'incorrectFixedBinaryLength': 0}],\n",
       "   'deleteVertex': [],\n",
       "   'deleteEdge': []}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data. The data files are in the same repo with this notebook\n",
    "print(conn.runLoadingJobWithFile(\"./data/cora/nodes.csv\", \"node_csv\", \"load_cora_data\"))\n",
    "print(conn.runLoadingJobWithFile(\"./data/cora/edges.csv\", \"edge_csv\", \"load_cora_data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add1757-1b0a-4c77-afee-eca3d3171643",
   "metadata": {},
   "source": [
    "If every step above finished successfully, now you should see the Cora graph in your GraphStudio. There are should be 2708 vertices and 10556 edges. For visual inspection, go to `Explore Graph` and pick a few vertices at random to see what they look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af0614-12ac-4c11-aa21-fcc16d0ab714",
   "metadata": {},
   "source": [
    "## Graph Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a66570",
   "metadata": {},
   "source": [
    "### PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be94bf5-775e-4af6-8f24-5fee9bfa2002",
   "metadata": {},
   "source": [
    "Install pagerank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42d7776-f9ad-4408-9add-1384758ba8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing and optimizing the queries, it might take a minute\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tg_pagerank'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer = conn.gds.featurizer()\n",
    "featurizer.installAlgorithm(\"tg_pagerank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45d447-7d2f-4066-a5d2-06da8d5c4c10",
   "metadata": {},
   "source": [
    "Run pagerank and get results with HttpReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17575abb-88d7-41ac-8a30-d065002cf897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import HttpReader, IterableWrapper\n",
    "\n",
    "url = \"https://torchdata-demo.i.tgcloud.io:443/restpp/query/Cora/tg_pagerank\"\n",
    "payload = {\n",
    "    \"v_type\": \"Paper\",\n",
    "    \"e_type\": \"Cite\",\n",
    "    \"top_k\": 2708,\n",
    "    \"print_accum\": True\n",
    "}\n",
    "authHeader = {'Authorization': \"Bearer \" + apiToken}\n",
    "out_pr = HttpReader(\n",
    "    IterableWrapper([url]),\n",
    "    None,\n",
    "    params=payload, headers=authHeader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdaf08",
   "metadata": {},
   "source": [
    "Reformat output from HttpReader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e11cb9-05c9-41b8-ac15-cd8e92ac5951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Vertex_ID': '1358', 'score': 33.06401}, {'Vertex_ID': '1701', 'score': 16.8922}, {'Vertex_ID': '1986', 'score': 14.46646}, {'Vertex_ID': '306', 'score': 13.72521}, {'Vertex_ID': '1810', 'score': 9.81972}]\n"
     ]
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "import torch.utils.data \n",
    "import json\n",
    "\n",
    "@torch.utils.data.functional_datapipe('process_data')\n",
    "class HttpReader_processing(IterDataPipe):\n",
    "    # A custom DataPipe to load and parse mesh data into PyTorch data objects.\n",
    "    def __init__(self, out: IterDataPipe):\n",
    "        super().__init__()\n",
    "        self.out = out\n",
    "\n",
    "    def __iter__(self):\n",
    "        reader_dp = self.out.readlines()\n",
    "        it = iter(reader_dp)\n",
    "        path, line = next(it)\n",
    "\n",
    "        out = json.loads(line.decode(\"utf8\"))\n",
    "    \n",
    "        yield out\n",
    "\n",
    "out_pr = out_pr.process_data()\n",
    "out_pagerank = next(iter(out_pr))\n",
    "print(out_pagerank['results'][0]['@@top_scores_heap'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0191940-4ee8-4427-8dd6-9ee5cb28a43c",
   "metadata": {},
   "source": [
    "Create a dataframe with two columns: vid and pageran score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2326d1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1358</td>\n",
       "      <td>33.06401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701</td>\n",
       "      <td>16.89220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986</td>\n",
       "      <td>14.46646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>13.72521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1810</td>\n",
       "      <td>9.81972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vid  pagerank\n",
       "0  1358  33.06401\n",
       "1  1701  16.89220\n",
       "2  1986  14.46646\n",
       "3   306  13.72521\n",
       "4  1810   9.81972"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pr = pd.DataFrame.from_records(out_pagerank['results'][0]['@@top_scores_heap'])\n",
    "df_pr.columns = [\"vid\", \"pagerank\"]\n",
    "df_pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea08e5-1bf0-434f-ae7a-90c951b8b3b3",
   "metadata": {},
   "source": [
    "### Other node features + labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb895b",
   "metadata": {},
   "source": [
    "Install the query that pulls node features `x`, `y`, `train_mask`, `val_mask` and `test_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aefe5b9-1bf4-4b2f-bfcc-df6cc9796da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExprFunctions installed successfully\n",
      "Using graph 'Cora'\n",
      "Successfully created queries: [vertex_loader].\n",
      "Start installing queries, about 1 minute ...\n",
      "vertex_loader query: curl -X GET 'https://127.0.0.1:9000/query/Cora/vertex_loader?input_vertices[INDEX]=VALUE&input_vertices[INDEX].type=VERTEX_TYPE&[num_batches=VALUE]&[shuffle=VALUE]&filter_by=VALUE'. Add -H \"Authorization: Bearer TOKEN\" if authentication is enabled.\n",
      "Select 'm1' as compile server, now connecting ...\n",
      "Node 'm1' is prepared as compile server.\n",
      "\n",
      "Query installation finished.\n"
     ]
    }
   ],
   "source": [
    "# Intall UDFs\n",
    "ExprFunctions=\"https://tg-mlworkbench.s3.us-west-1.amazonaws.com/udf/1.0/ExprFunctions.hpp\" \n",
    "ExprUtil=\"\" \n",
    "conn.installUDF(ExprFunctions, ExprUtil)\n",
    "\n",
    "# Create query to get data\n",
    "query = \"\"\"\n",
    "USE GRAPH Cora\n",
    "\n",
    "CREATE QUERY vertex_loader(\n",
    "    SET<VERTEX> input_vertices,\n",
    "    INT num_batches=1, \n",
    "    BOOL shuffle=FALSE,\n",
    "    STRING filter_by\n",
    "){\n",
    "    /*\n",
    "    This query generates batches of vertices. If `input_vertices` is given, it will generate \n",
    "    a batches of those vertices. Otherwise, it will divide all vertices into `num_batches`, \n",
    "    and return each batch separately.\n",
    "\n",
    "    Parameters :\n",
    "      input_vertices : What vertices to get.\n",
    "      num_batches    : Number of batches to divide all vertices.\n",
    "      shuffle        : Whether to shuffle vertices before collecting data.\n",
    "      filter_by      : A Boolean attribute to determine which vertices are included.\n",
    "                       Only effective when `input_vertices` is NULL.\n",
    "    */\n",
    "    INT num_vertices;\n",
    "    SumAccum<INT> @tmp_id;\n",
    "\n",
    "    # Shuffle vertex ID if needed\n",
    "    start = {ANY};\n",
    "    IF shuffle THEN\n",
    "        num_vertices = start.size();\n",
    "        res = SELECT s \n",
    "              FROM start:s\n",
    "              POST-ACCUM s.@tmp_id = floor(rand()*num_vertices);\n",
    "    ELSE\n",
    "        res = SELECT s \n",
    "              FROM start:s\n",
    "              POST-ACCUM s.@tmp_id = getvid(s);\n",
    "    END;\n",
    "\n",
    "    # Generate batches\n",
    "    FOREACH batch_id IN RANGE[0, num_batches-1] DO\n",
    "        MapAccum<VERTEX, STRING> @@v_batch;\n",
    "        IF input_vertices.size()==0 THEN\n",
    "            start = {ANY};\n",
    "            IF filter_by IS NOT NULL THEN\n",
    "                seeds = SELECT s \n",
    "                        FROM start:s \n",
    "                        WHERE s.getAttr(filter_by, \"BOOL\") and s.@tmp_id % num_batches == batch_id\n",
    "                        POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "            ELSE\n",
    "                seeds = SELECT s \n",
    "                        FROM start:s \n",
    "                        WHERE s.@tmp_id % num_batches == batch_id\n",
    "                        POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "            END;\n",
    "        ELSE\n",
    "            start = input_vertices;\n",
    "            seeds = SELECT s \n",
    "                    FROM start:s \n",
    "                    POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "        END;\n",
    "        # Add to response\n",
    "        PRINT @@v_batch AS vertex_batch;  \n",
    "    END;\n",
    "}\n",
    "\n",
    "INSTALL QUERY vertex_loader\n",
    "\"\"\"\n",
    "print(conn.gsql(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3323a18",
   "metadata": {},
   "source": [
    "Run the query and get results with HttpReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af11907-c775-41ab-8a38-bc173e65462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://torchdata-demo.i.tgcloud.io:443/restpp/query/Cora/vertex_loader\"\n",
    "out = HttpReader(\n",
    "    IterableWrapper([url]),\n",
    "    headers=authHeader)\n",
    "\n",
    "out = out.process_data()\n",
    "out_features = next(iter(out))\n",
    "out_features = out_features[\"results\"][0][\"vertex_batch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be5626-d5ef-4af4-b788-0232bfccf639",
   "metadata": {},
   "source": [
    "Create a dataframe to store all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "036ca0e8-3828-4ee8-99df-47f324154a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>train_mask</th>\n",
       "      <th>val_mask</th>\n",
       "      <th>test_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1031</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>959</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vid                                                  x  y  train_mask  \\\n",
       "0  1005  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "1  1031  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "2  1656  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1           0   \n",
       "3   959  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "4   846  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "\n",
       "   val_mask  test_mask  \n",
       "0         0          1  \n",
       "1         0          1  \n",
       "2         0          1  \n",
       "3         0          1  \n",
       "4         0          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vids, x, y, train_mask, val_mask, test_mask = [], [], [], [], [], []\n",
    "for vid, line in out_features.items():\n",
    "    vids.append(vid)\n",
    "    split_line = line.strip().split(\",\")\n",
    "    x.append([int(i) for i in split_line[1].strip().split()])\n",
    "    y.append(int(split_line[2]))\n",
    "    train_mask = int(split_line[3])\n",
    "    val_mask = int(split_line[4])\n",
    "    test_mask = int(split_line[5])\n",
    "\n",
    "df_td = pd.DataFrame({\n",
    "    \"vid\": vids, \n",
    "    \"x\": x, \n",
    "    \"y\": y, \n",
    "    \"train_mask\": train_mask, \n",
    "    \"val_mask\": val_mask, \n",
    "    \"test_mask\": test_mask\n",
    "})\n",
    "\n",
    "df_td.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd4336",
   "metadata": {},
   "source": [
    "Merge with pagerank to get the dataframe for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f260698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>train_mask</th>\n",
       "      <th>val_mask</th>\n",
       "      <th>test_mask</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1005</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1031</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.74351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1656</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.62322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>959</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vid                                                  x  y  train_mask  \\\n",
       "0  1005  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "1  1031  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "2  1656  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1           0   \n",
       "3   959  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "4   846  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  3           0   \n",
       "\n",
       "   val_mask  test_mask  pagerank  \n",
       "0         0          1   1.00000  \n",
       "1         0          1   0.74351  \n",
       "2         0          1   0.62322  \n",
       "3         0          1   1.00000  \n",
       "4         0          1   0.90899  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paper = df_td.merge(df_pr, on=\"vid\")\n",
    "df_paper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb5026-3223-41ac-87a4-b7ca2c4044ce",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "Use torchdata to build data loaders for splitting, shuffling, batching, collating, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06f990d-8153-4b3b-97bc-25b2da299969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Percentages of data for training, validation, and testing respectively.\n",
    "train_perc = 0.7\n",
    "valid_perc = 0.15\n",
    "\n",
    "# Define sample function for train/validation/test split\n",
    "def sample_fn(n):\n",
    "    r = random.random()\n",
    "    if r<=train_perc:\n",
    "        return 0\n",
    "    elif r<train_perc+valid_perc:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19fe6820-8c9c-4698-b8f9-6b6aaa97ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define collate function\n",
    "def coll_fn(batch):\n",
    "    xs = [sample[0] + [sample[1]] for sample in batch]\n",
    "    ys = [sample[2] for sample in batch] \n",
    "    return torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a7d0a-3611-4884-86e6-8cc165d90ece",
   "metadata": {},
   "source": [
    "Creating our own data_loader function that applies a Batcher, Shuffler, and Collator. Using these data primitives, the data loader \n",
    "is easily customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eff4af71-5b16-4a76-bd8d-9512ff25ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import Zipper\n",
    "\n",
    "def data_loader(x, x2, y, shuffle, batch_sz, collator_fn=coll_fn, sample_fn=sample_fn):\n",
    "    data_x = IterableWrapper(x)\n",
    "    data_x2 = IterableWrapper(x2)\n",
    "    data_y = IterableWrapper(y)\n",
    "    data = Zipper(data_x, data_x2, data_y)\n",
    "\n",
    "    train_set, valid_set, test_set = data.demux(\n",
    "        num_instances=3, classifier_fn=sample_fn)\n",
    "\n",
    "    train_set = train_set.batch(batch_sz).collate(coll_fn)\n",
    "    valid_set = valid_set.batch(batch_sz).collate(coll_fn)\n",
    "    test_set = test_set.batch(batch_sz).collate(coll_fn)\n",
    "\n",
    "    if shuffle:\n",
    "        train_set = train_set.shuffle()\n",
    "        valid_set = valid_set.shuffle()\n",
    "        test_set = test_set.shuffle()\n",
    "\n",
    "    return train_set, valid_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9d2b28-4256-4c9e-819c-a5f0f5a502bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "batch_size = 100\n",
    "train_set, valid_set, test_set = data_loader(\n",
    "    df_paper.x, df_paper.pagerank, df_paper.y, shuffle, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb62d2-c798-4746-bede-aeaae36290a3",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8404fd6",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79445a-6f6c-4f09-b87d-a126cb51aaa8",
   "metadata": {},
   "source": [
    "Define a simple feedforward network that has 2 linear hidden layers and applies the ReLU non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0006869f-4f69-46e1-a94c-c21e1cace88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class simple_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(simple_NN, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.double()\n",
    "        sz = x.size()[1]\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b2d9922-0d87-4b13-b0e7-12e9640ca34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_NN(\n",
       "  (linear1): Linear(in_features=1434, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(df_paper.x.iloc[0]) + 1 \n",
    "hidden_dim = 256\n",
    "output_size = len(df_paper.y.unique())\n",
    "\n",
    "model = simple_NN(input_size, hidden_dim, output_size)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d3cd5-3397-4c07-b141-279588afc011",
   "metadata": {},
   "source": [
    "Choose optimizer algorithm (ADAM) and loss function (Cross Entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfda499-e08a-49ec-940c-492b97dd77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0676e1dc-3aa7-4a12-b71d-d470147d915a",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78ee8d",
   "metadata": {},
   "source": [
    "Define the validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b69433e-906c-4530-999c-e0368e1e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    total = 0\n",
    "    for data, target in dataloader:\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            val_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            val_acc += (pred==target).sum().item()\n",
    "            total += len(target)\n",
    "    val_loss /= total\n",
    "    val_acc /=total\n",
    "    print('Validation, loss: {:.6f}, accuracy: {:.6f}'.format(val_loss, val_acc))\n",
    "\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef359e12",
   "metadata": {},
   "source": [
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b699046-6d1d-4d3e-a70e-5c0fb713b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = output.argmax(dim=1)\n",
    "            train_acc.append(float((predicted == target).sum())/len(target))\n",
    "        \n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {}, batch #: {}, loss: {:.6f}, accuracy: {:.6f}'.format(\n",
    "                    epoch, batch_idx, loss.item(), train_acc[-1]))\n",
    "    \n",
    "        val_acc.append(validate(model, val_data))\n",
    "        \n",
    "    return train_acc, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18213ec3-1352-4d67-9700-9dd581af98ab",
   "metadata": {},
   "source": [
    "Train our neural network and print the training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "201e1e35-0456-4e65-91d1-9eb5b6b60da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feng.shi/mlworkbench/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/combining.py:248: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0, batch #: 0, loss: 1.949416, accuracy: 0.170000\n",
      "Train Epoch: 0, batch #: 10, loss: 1.829437, accuracy: 0.300000\n",
      "Validation, loss: 1.663495, accuracy: 0.304136\n",
      "Train Epoch: 1, batch #: 0, loss: 1.608917, accuracy: 0.350000\n",
      "Train Epoch: 1, batch #: 10, loss: 1.574102, accuracy: 0.390000\n",
      "Validation, loss: 1.209667, accuracy: 0.681818\n",
      "Train Epoch: 2, batch #: 0, loss: 1.239762, accuracy: 0.660000\n",
      "Train Epoch: 2, batch #: 10, loss: 0.910784, accuracy: 0.810000\n",
      "Validation, loss: 0.860516, accuracy: 0.744949\n",
      "Train Epoch: 3, batch #: 0, loss: 0.720289, accuracy: 0.800000\n",
      "Train Epoch: 3, batch #: 10, loss: 0.663243, accuracy: 0.863158\n",
      "Validation, loss: 0.584671, accuracy: 0.857831\n",
      "Train Epoch: 4, batch #: 0, loss: 0.520550, accuracy: 0.900000\n",
      "Train Epoch: 4, batch #: 10, loss: 0.508892, accuracy: 0.880000\n",
      "Validation, loss: 0.428960, accuracy: 0.900726\n",
      "Train Epoch: 5, batch #: 0, loss: 0.481546, accuracy: 0.850000\n",
      "Train Epoch: 5, batch #: 10, loss: 0.346480, accuracy: 0.930000\n",
      "Validation, loss: 0.358150, accuracy: 0.907090\n",
      "Train Epoch: 6, batch #: 0, loss: 0.351641, accuracy: 0.950000\n",
      "Train Epoch: 6, batch #: 10, loss: 0.265295, accuracy: 0.916667\n",
      "Validation, loss: 0.272470, accuracy: 0.946015\n",
      "Train Epoch: 7, batch #: 0, loss: 0.270551, accuracy: 0.930000\n",
      "Train Epoch: 7, batch #: 10, loss: 0.245943, accuracy: 0.930000\n",
      "Validation, loss: 0.224708, accuracy: 0.956416\n",
      "Train Epoch: 8, batch #: 0, loss: 0.187590, accuracy: 0.970000\n",
      "Train Epoch: 8, batch #: 10, loss: 0.168068, accuracy: 0.960000\n",
      "Validation, loss: 0.205722, accuracy: 0.960733\n",
      "Train Epoch: 9, batch #: 0, loss: 0.161926, accuracy: 0.980000\n",
      "Train Epoch: 9, batch #: 10, loss: 0.118324, accuracy: 1.000000\n",
      "Validation, loss: 0.159123, accuracy: 0.970822\n"
     ]
    }
   ],
   "source": [
    "train_acc, val_acc = train(model, train_set, valid_set, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c9e92a",
   "metadata": {},
   "source": [
    "Compute the testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c444746-7a21-4dbe-a6cb-faeae030ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation, loss: 0.148143, accuracy: 0.975000\n",
      "Final test accuracy is 0.975\n"
     ]
    }
   ],
   "source": [
    "test_acc = validate(model, test_set)\n",
    "print(\"Final test accuracy is {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "96daeecb52bbbb8e3aef04d2f9c6a1e01f271d07cea30059f3c558ef00b717d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
