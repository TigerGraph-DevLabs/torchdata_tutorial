{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d906cb6-883e-4007-84db-c0b8cdc575d4",
   "metadata": {},
   "source": [
    "# Exploring how the TorchData API works with TigerGraph data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba21f58-89a4-46d0-98d5-eec4247b12a8",
   "metadata": {},
   "source": [
    "### by Doris Voina (dorisvoina@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d342d6-5d93-4c5a-983f-bff2ae0fffd6",
   "metadata": {},
   "source": [
    "#### What is TorchData?\n",
    "#### TorchData (or \"torchdata\") is part of Pytorch 1.11 and enables data loading and building (see [link to documentation](https://pytorch.org/data/main/tutorial.html)) . According to the documentation, \n",
    ">TorchData is a library of common modular data loading primitives for easily constructing flexible and performant data pipelines.\n",
    "\n",
    "#### The modular data loading primitives of torchdata are capable of accomplishing a variety of functions:\n",
    "- FileLister: lists out files in a directory\n",
    "- Filter: filters the elements in DataPipe based on a given function\n",
    "- FileOpener: consumes file paths and returns opened file streams\n",
    "- Mapper: Applies a function over each item from the source DataPipe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef02fa-3511-4561-b227-4cea89724878",
   "metadata": {},
   "source": [
    "**This example is a tutorial that showcases how to use a TigerGraph dataset with a data loading implementation using PyTorch's torchdata in order to solve a Machine Learning problem with Cora, a well-known graph dataset of papers and their citations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e81e3-6d77-483e-ab47-1de028c85ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c90f895-3ffa-4c69-bc0f-6f35eca39f13",
   "metadata": {},
   "source": [
    "Let's use torchdata by looking at a particular problem: \n",
    "starting with a graph, a common problem is classifying nodes of the graph. A common approach is to consider node features and then classify nodes according to these features, using say a neural network. While we can use the features provided in the dataset, we can further enrich these features by adding node properties in the graph, such as for instance pagerank, a property whereby the number and quality of conenctions to a node are counted in order to determine a rough estimate of how important the node is. The underlying assumption of pagerank is that more important nodes are likely to receive more connections from other nodes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05a74b-0e2e-4a0f-bd94-d5dcc8d79f59",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "- import the Cora dataset from TigerGraph using torchdata's HttpReader: data is node features (occurrence of words in a paper), the pagerank feature, and labels; \n",
    "- build a data loder by shuffling, batching, collating, etc. data\n",
    "- train a simple feedforward neural network on the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c54b27-3f2d-4153-80c8-cb9de9598223",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Let's dig in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2477271c-431c-4031-a365-f3829de00c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tigergraph/miniforge3/envs/gds_dev2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyTigerGraph as tg\n",
    "\n",
    "#import tgml\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchdata\n",
    "from torchdata.datapipes.iter import (\n",
    "    IterableWrapper,\n",
    "    IterKeyZipper,\n",
    "    IterDataPipe,\n",
    "    HttpReader,\n",
    "    Zipper,\n",
    "    Mapper,\n",
    "    Shuffler,\n",
    "    Sampler,\n",
    "    Batcher,\n",
    "    Collator\n",
    ")\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa558c9-bca0-484e-bcb1-8b5a15ca53bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29af0614-12ac-4c11-aa21-fcc16d0ab714",
   "metadata": {},
   "source": [
    "## Get features: pagerank + other features...\n",
    "### Read the pagerank feature with torchdata's HttpReader*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45d447-7d2f-4066-a5d2-06da8d5c4c10",
   "metadata": {},
   "source": [
    "*see Appendix about the GSQL queries used with HttpReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9cb3ca-7dc0-41b9-9f61-0e3e465c1808",
   "metadata": {},
   "source": [
    "set parameters of pagerank algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28709345-a760-4d47-9bff-963c3983800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'v_type': 'PAPER', 'e_type': 'CITE', 'max_change': 0.001, 'max_iter': 25, 'damping': 0.85,\n",
    "         'top_k': 100, 'print_accum': True, 'result_attr':'pagerank','file_path':'','display_edges': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17575abb-88d7-41ac-8a30-d065002cf897",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pr = HttpReader(IterableWrapper([\"http://35.230.92.92:14240/restpp/query/Cora/tg_pagerank?v_type=Paper&e_type=Cite&max_change=0.001&max_iter=25&damping=0.85&top_k=2708&print_accum=True&result_attr=pagerank&display_edges=False\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e11cb9-05c9-41b8-ac15-cd8e92ac5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.utils.data.functional_datapipe('process_data')\n",
    "class HttpReader_processing(IterDataPipe):\n",
    "    # A custom DataPipe to load and parse mesh data into PyG data objects.\n",
    "    def __init__(self, out: IterDataPipe):\n",
    "        super().__init__()\n",
    "        self.out = out\n",
    "\n",
    "    def __iter__(self):\n",
    "            \n",
    "        reader_dp = self.out.readlines()\n",
    "        it = iter(reader_dp)\n",
    "        path, line = next(it)\n",
    "\n",
    "        out = json.loads(line.decode(\"utf8\"))\n",
    "    \n",
    "        yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee63293d-cd25-47f0-8878-aab00e9bb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pr = out_pr.process_data()\n",
    "out_pagerank = next(iter(out_pr))\n",
    "print(out_pagerank['results'][0]['@@top_scores_heap'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0191940-4ee8-4427-8dd6-9ee5cb28a43c",
   "metadata": {},
   "source": [
    "create dictionary with key = vertex ID, value = real scalar which is pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e2e6d-3b0a-4b48-87b7-c2dd4b615718",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pr = out_pagerank['results'][0]['@@top_scores_heap']\n",
    "\n",
    "dict_pr = {}\n",
    "for d in list_of_pr:\n",
    "    dict_pr[d[\"Vertex_ID\"]] = d[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea08e5-1bf0-434f-ae7a-90c951b8b3b3",
   "metadata": {},
   "source": [
    "### Read other node features + labels with torchdata's HttpReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af11907-c775-41ab-8a38-bc173e65462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = HttpReader(IterableWrapper([\"http://35.230.92.92:14240/restpp/query/Cora/vertex_hloader_x_y_train_mask_val_mask_test_mask\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6d80c-4d6a-4ea1-a018-55281a54b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.process_data()\n",
    "out_features = next(iter(out))\n",
    "out_features = out_features[\"results\"][0][\"vertex_batch\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be5626-d5ef-4af4-b788-0232bfccf639",
   "metadata": {},
   "source": [
    "create a pandas dataframe to be used in the neural network later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ca0e8-3828-4ee8-99df-47f324154a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td = pd.DataFrame(columns = [\"v_id\", \"x\", \"y\", \"train_mask\", \"val_mask\", \"test_mask\", \"pagerank\"])\n",
    "\n",
    "for ind, v_id in enumerate(out_features):\n",
    "    \n",
    "    split_line = out_features[v_id][:-1].split(\",\")\n",
    "    x_line = split_line[1][:-1]\n",
    "    x = [int(x_line.split(\" \")[i]) for i in range(len(x_line.split(\" \")))]\n",
    "    y = int(split_line[2])\n",
    "    train_mask = int(split_line[5])\n",
    "    val_mask = int(split_line[4])\n",
    "    test_mask = int(split_line[3])\n",
    "    \n",
    "    df_td.loc[ind] = [v_id, x, y, train_mask, val_mask, test_mask, dict_pr[v_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd305db-a32c-42dd-960b-c7bfd58c452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862abce-8526-4048-b81d-90ce850bfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paper = df_td"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb5026-3223-41ac-87a4-b7ca2c4044ce",
   "metadata": {},
   "source": [
    "## Using torchdata for shuffling, batching, collating, etc.\n",
    "### a specific example of using it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d1ff2-2aa0-408b-98d4-f3ee2b7a8f63",
   "metadata": {},
   "source": [
    "choose how much percentage of the data is training and testing, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f990d-8153-4b3b-97bc-25b2da299969",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = 0.7\n",
    "valid_perc = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe6820-8c9c-4698-b8f9-6b6aaa97ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coll_fn(batch):\n",
    "    \n",
    "    xs = [sample[0] for sample in batch]\n",
    "    ys = [sample[1] for sample in batch] \n",
    "    \n",
    "    return torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ad8e7-7a51-4b8b-afac-cb3475810cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_fn(n):\n",
    "    r = random.random()\n",
    "    if r<=train_perc:\n",
    "        return 0\n",
    "    elif r<train_perc+valid_perc:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458a7d0a-3611-4884-86e6-8cc165d90ece",
   "metadata": {},
   "source": [
    "Creating our own data_loader function that applies a Batcher, Shuffler, and Collator. Using these data primitives, the data loader \n",
    "is easily customizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4af71-5b16-4a76-bd8d-9512ff25ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_x, data_y, shuffle, batch_sz, collator_fn = coll_fn):   #working\n",
    "\n",
    "    df_x = IterableWrapper(data_x)\n",
    "    df_y = IterableWrapper(data_y)\n",
    "    data_xy = Zipper(df_x, df_y)\n",
    "    \n",
    "    train_set, valid_set, test_set = data_xy.demux(num_instances=3, classifier_fn=sample_fn)\n",
    "\n",
    "    data_xy = data_xy.batch(batch_sz).collate(coll_fn)\n",
    "    train_set = train_set.batch(batch_sz).collate(coll_fn)\n",
    "    valid_set = valid_set.batch(batch_sz).collate(coll_fn)\n",
    "    test_set = test_set.batch(batch_sz).collate(coll_fn)\n",
    "    \n",
    "    if shuffle:\n",
    "        data_xy = data_xy.shuffle()\n",
    "        train_set = train_set.shuffle()\n",
    "        valid_set = valid_set.shuffle()\n",
    "        test_set = test_set.shuffle()\n",
    "         \n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87d4f9-66a9-428b-a0a7-b4ec28a9ba38",
   "metadata": {},
   "source": [
    "add pagerank feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f07047-5f68-43ad-8ab3-2571cfc22c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = {'x': df_paper.x}\n",
    "df_x = pd.DataFrame(frame) #save in new dataframe so as not to modify df_paper (in case we need it later)\n",
    "\n",
    "list_feature = []\n",
    "for pr, x in zip(df_paper.pagerank, df_x.x):\n",
    "    x.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d2b28-4256-4c9e-819c-a5f0f5a502bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "batch_size = 5\n",
    "train_set, valid_set, test_set = data_loader(df_x.x, df_paper.y, shuffle, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb62d2-c798-4746-bede-aeaae36290a3",
   "metadata": {},
   "source": [
    "# Train simple linear neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f1baf-ebd1-46cb-8175-b30776bcbd8a",
   "metadata": {},
   "source": [
    "### on data with graph feature pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d79445a-6f6c-4f09-b87d-a126cb51aaa8",
   "metadata": {},
   "source": [
    "create a simple feedforward network that has 2 linear hidden layers and applies the ReLU non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006869f-4f69-46e1-a94c-c21e1cace88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_NN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(simple_NN, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.double()\n",
    "        sz = x.size()[1]\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba106352-9bba-44d0-b4e7-6afb77e34d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dca7f-604a-46aa-b2e6-f9c4e819127d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d9922-0d87-4b13-b0e7-12e9640ca34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(df_x.x.iloc[0])\n",
    "hidden_dim = 500\n",
    "output_size = len(df_paper.y.unique())\n",
    "\n",
    "model = simple_NN(input_size, hidden_dim, output_size)\n",
    "\n",
    "model.double()\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca627a31-d4c1-4ea2-86bd-befc10ea82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "seed = 15\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1ba40-e552-4889-8f9c-5ab50eaaefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_path = \"models_and_results\"\n",
    "project = \"cora_classification_w/_simple_net\"\n",
    "\n",
    "save_path =  '/'.join([save_path, ])\n",
    "if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8341a48-9879-4a17-b234-d63adb8e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d3d3cd5-3397-4c07-b141-279588afc011",
   "metadata": {},
   "source": [
    "choose optimizer algorithm (ADAM) and loss function (Cross Entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfda499-e08a-49ec-940c-492b97dd77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2daa740-53d9-426e-90b7-7f9fc6a6b13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0676e1dc-3aa7-4a12-b71d-d470147d915a",
   "metadata": {},
   "source": [
    "training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b699046-6d1d-4d3e-a70e-5c0fb713b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataT, dataV, epochs):\n",
    "    model.train()\n",
    "\n",
    "    acc_test_duringL = []\n",
    "    acc_train_duringL = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(dataT):\n",
    "            if cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            predicted = torch.max(output.data, 1)[1]\n",
    "            loss = error(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc_train_duringL.append(float((predicted.to(device) == target).sum())/len(target))\n",
    "        \n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Train Epoch: {} batch #: {}, accuracy: {:.6f}, Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, float((predicted.to(device) == target).sum())/len(target), loss.item()))\n",
    "    \n",
    "                #acc = validate(model, dataV)\n",
    "                #acc_test_duringL.append(acc)\n",
    "            \n",
    "        \n",
    "    return acc_train_duringL\n",
    "    #, acc_test_duringL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69433e-906c-4530-999c-e0368e1e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataV):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "            \n",
    "    for data, target in dataV:\n",
    "        \n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        val_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "        total += len(target)\n",
    "\n",
    "    val_loss /= total\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, total,\n",
    "        100. * correct / total))\n",
    "\n",
    "    return 100. * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18213ec3-1352-4d67-9700-9dd581af98ab",
   "metadata": {},
   "source": [
    "training our feedforwrd neural network and printing the test data accuracy at the end..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e1e35-0456-4e65-91d1-9eb5b6b60da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_duringL = train(model, train_set, test_set, 1)\n",
    "acc_test = validate(model, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c444746-7a21-4dbe-a6cb-faeae030ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"final test accuracy is {}%!\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f515c17-26f5-49df-a82b-e378e5006d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76962dc2-4f06-43e1-bbd2-e4c7acbf39f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bbdd57a-94a5-42ee-99be-583947ba0eb4",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0b9e9-dc05-4fed-999f-7fc88c97f50f",
   "metadata": {},
   "source": [
    "### GSQL query: tg_pagerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf44a7-5c9a-4139-8721-eadb11c66cdc",
   "metadata": {},
   "source": [
    "```\n",
    "CREATE QUERY tg_pagerank (STRING v_type, STRING e_type,\n",
    " FLOAT max_change=0.001, INT max_iter=25, FLOAT damping=0.85, INT top_k = 100,\n",
    " BOOL print_accum = TRUE, STRING result_attr =  \"\", STRING file_path = \"\",\n",
    " BOOL display_edges = FALSE) SYNTAX V2 {\n",
    " ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53bafa-0a2f-455a-8976-c2699b2b5743",
   "metadata": {
    "tags": []
   },
   "source": [
    "/*\n",
    " Compute the pageRank score for each vertex in the GRAPH\n",
    " In each iteration, compute a score for each vertex:\n",
    "     score = (1-damping) + damping*sum(received scores FROM its neighbors).\n",
    " The pageRank algorithm stops when either of the following is true:\n",
    " a) it reaches max_iter iterations;\n",
    " b) the max score change for any vertex compared to the last iteration <= max_change.\n",
    " v_type: vertex types to traverse          print_accum: print JSON output\n",
    " e_type: edge types to traverse            result_attr: INT attr to store results to\n",
    " max_iter; max #iterations                 file_path: file to write CSV output to\n",
    " top_k: #top scores to output              display_edges: output edges for visualization\n",
    " max_change: max allowed change between iterations to achieve convergence\n",
    " damping: importance of traversal vs. random teleport\n",
    "\n",
    " This query supports only taking in a single edge for the time being (8/13/2020).\n",
    "*/\n",
    "\n",
    "```\n",
    "TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;   \\  \n",
    "HeapAccum<Vertex_Score>(top_k, score DESC) @@top_scores_heap;  \n",
    "MaxAccum<FLOAT> @@max_diff = 9999;    # max score change in an iteration  \n",
    "SumAccum<FLOAT> @sum_recvd_score = 0; # sum of scores each vertex receives FROM neighbors  \n",
    "SumAccum<FLOAT> @sum_score = 1;           # initial score for every vertex is 1.  \n",
    "SetAccum<EDGE> @@edge_set;             # list of all edges, if display is needed  \n",
    "FILE f (file_path);  \n",
    "```\n",
    "\n",
    "#PageRank iterations\n",
    "\n",
    "```\n",
    "Start = {v_type};                     # Start with all vertices of specified type(s)\n",
    "WHILE @@max_diff > max_change \n",
    "    LIMIT max_iter DO\n",
    "        @@max_diff = 0;\n",
    "    V = SELECT s\n",
    "\tFROM Start:s -(e_type:e)- v_type:t\n",
    "\tACCUM \n",
    "            t.@sum_recvd_score += s.@sum_score/(s.outdegree(e_type)) \n",
    "\tPOST-ACCUM \n",
    "            s.@sum_score = (1.0-damping) + damping * s.@sum_recvd_score,\n",
    "\t    s.@sum_recvd_score = 0,\n",
    "\t    @@max_diff += abs(s.@sum_score - s.@sum_score');\n",
    "END; # END WHILE loop`\n",
    "```\n",
    "\n",
    "#Output\n",
    "\n",
    "```\n",
    "IF file_path != \"\" THEN\n",
    "    f.println(\"Vertex_ID\", \"PageRank\");\n",
    "END;\n",
    "V = SELECT s \n",
    "    FROM Start:s\n",
    "    POST-ACCUM \n",
    "        IF result_attr != \"\" THEN \n",
    "            s.setAttr(result_attr, s.@sum_score) \n",
    "        END,\n",
    "   \n",
    "    IF file_path != \"\" THEN \n",
    "            f.println(s, s.@sum_score) \n",
    "        END,\n",
    "  \n",
    "    IF print_accum THEN \n",
    "            @@top_scores_heap += Vertex_Score(s, s.@sum_score) \n",
    "        END;\n",
    "\n",
    "IF print_accum THEN\n",
    "    PRINT @@top_scores_heap;\n",
    "    IF display_edges THEN\n",
    "        PRINT Start[Start.@sum_score];\n",
    "    Start = SELECT \n",
    "        FROM Start:s -(e_type:e)- v_type:t\n",
    "            ACCUM @@edge_set += e;\n",
    "        PRINT @@edge_set;\n",
    "    END;\n",
    "END;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec378fe-536d-4588-9eb8-fd7e894df912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "680cf022-5713-4f8c-ac03-22bb5f04e4e5",
   "metadata": {},
   "source": [
    "### GSQL query to load node features and labels: vertex_hloader_x_y_train_mask_val_mask_test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21d1b8f-c0a3-4530-988d-fe9b6ea12b75",
   "metadata": {},
   "source": [
    "```\n",
    "CREATE QUERY vertex_hloader_x_y_train_mask_val_mask_test_mask(\n",
    "    SET<VERTEX> input_vertices,\n",
    "    INT num_batches=1, \n",
    "    BOOL shuffle=FALSE,\n",
    "    STRING filter_by\n",
    "){\n",
    "    /*\n",
    "    This query generates batches of vertices. If `input_vertices` is given, it will generate \n",
    "    a batches of those vertices. Otherwise, it will divide all vertices into `num_batches`, \n",
    "    and return each batch separately.\n",
    "\n",
    "    Parameters :\n",
    "      input_vertices : What vertices to get.\n",
    "      num_batches    : Number of batches to divide all vertices.\n",
    "      shuffle        : Whether to shuffle vertices before collecting data.\n",
    "      filter_by      : A Boolean attribute to determine which vertices are included.\n",
    "                       Only effective when `input_vertices` is NULL.\n",
    "    */\n",
    "    INT num_vertices;\n",
    "    SumAccum<INT> @tmp_id;\n",
    "\n",
    "    # Shuffle vertex ID if needed\n",
    "    start = {ANY};\n",
    "    IF shuffle THEN\n",
    "        num_vertices = start.size();\n",
    "        res = SELECT s \n",
    "              FROM start:s\n",
    "              POST-ACCUM s.@tmp_id = floor(rand()*num_vertices);\n",
    "    ELSE\n",
    "        res = SELECT s \n",
    "              FROM start:s\n",
    "              POST-ACCUM s.@tmp_id = getvid(s);\n",
    "    END;\n",
    "\n",
    "    # Generate batches\n",
    "    FOREACH batch_id IN RANGE[0, num_batches-1] DO\n",
    "        MapAccum<VERTEX, STRING> @@v_batch;\n",
    "        IF input_vertices.size()==0 THEN\n",
    "            start = {ANY};\n",
    "            IF filter_by IS NOT NULL THEN\n",
    "                seeds = SELECT s \n",
    "                        FROM start:s \n",
    "                        WHERE s.getAttr(filter_by, \"BOOL\") and s.@tmp_id % num_batches == batch_id\n",
    "                        POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "            ELSE\n",
    "                seeds = SELECT s \n",
    "                        FROM start:s \n",
    "                        WHERE s.@tmp_id % num_batches == batch_id\n",
    "                        POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "            END;\n",
    "        ELSE\n",
    "            start = input_vertices;\n",
    "            seeds = SELECT s \n",
    "                    FROM start:s \n",
    "                    POST-ACCUM @@v_batch += (s -> (int_to_string(getvid(s)) + \",\" + int_to_string(s.x)+\",\"+int_to_string(s.y)+\",\"+bool_to_string(s.train_mask)+\",\"+bool_to_string(s.val_mask)+\",\"+bool_to_string(s.test_mask) + \"\\n\"));\n",
    "        END;\n",
    "        # Add to response\n",
    "        PRINT @@v_batch AS vertex_batch;  \n",
    "    END;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c85ed5-a231-40f0-a3b3-518207df2ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e828fd6-fac5-4c9b-b8e6-a3cf7c8c911e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
